{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"table_of_contents\"></a>\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#intro)\n",
    "    * 1.1 [Problem Statement](#problem_statement)\n",
    "    * 1.2 [Data Gathering](#data_gathering)\n",
    "2. [Setup](#setup)\n",
    "    * 2.1 [Importing the Data](#import)\n",
    "3. [General](#general)\n",
    "    * 3.1 [Shape](#shape)\n",
    "    * 3.2 [Statistics](#stats)\n",
    "    * 3.3 [Hygiene](#hygiene)\n",
    "4. [Cohort Analysis](#cohort)\n",
    "    * 4.1 [Data Cleaning](#data_cleaning)\n",
    "    * 4.2 [Invoiceno Test](#invoiceno)\n",
    "    * 4.3 [Quantity/Price Test](#quantityprice)\n",
    "    * 4.4 [Completing Variables](#iddesc)\n",
    "    * 4.5 [Creating New Variables](#creating)\n",
    "    * 4.6 [Calculate Absolute Cohorts and Retention](#retention)\n",
    "    * 4.7 [Calculate Average Price](#price)\n",
    "    * 4.8 [Calculate Average Quantity](#quantity)\n",
    "5. [Cohort Analysis Visualization](#eda)\n",
    "6. [RFM Analysis](#rfm)\n",
    "    * 6.1 [RFM Scoring](#rfm_scoring)\n",
    "    * 6.2 [RFM Profiling](#rfm_profiling)\n",
    "7. [Segmentation](#segmentation)\n",
    "    * 7.1 [K-Means](#kmeans)\n",
    "    * 7.2 [Choosing K](#choosing_k)\n",
    "    * 7.3 [Scree Plot](#screeplot)\n",
    "    * 7.4 [K Clusters](#k_clusters)\n",
    "    * 7.5 [Snake Plots](#snake_plots)\n",
    "    * 7.6 [Importance Scores](#importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"problem_statement\"></a>\n",
    "\n",
    "## 1.1 Problem Statement\n",
    "\n",
    "**Describe the dataset?**\n",
    "\n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_gathering\"></a>\n",
    "\n",
    "## 1.2 Data Gathering\n",
    "\n",
    "The dataset from ?? dataset \n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "\n",
    "## 2. Setup\n",
    "\n",
    "**Packages required** \n",
    "* Data manipulation \n",
    "* Visualization \n",
    "* Modelling \n",
    "* Validation \n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulation  \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime as dt\n",
    "\n",
    "# Visualization \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "\n",
    "# ignore warnings \n",
    "# import warnings \n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# Modelling \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Preprocessing \n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder \n",
    "# For feature select\n",
    "from sklearn.feature_selection import SelectFromModel, RFECV\n",
    "\n",
    "# Validation\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, ShuffleSplit, cross_validate\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, roc_curve, roc_auc_score, precision_score, recall_score, plot_confusion_matrix\n",
    "\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#4878d0', '#ee854a', '#6acc64', '#d65f5f', '#956cb4', '#8c613c', '#dc7ec0', '#797979', '#d5bb67', '#82c6e2']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAABECAYAAACF4e8fAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAACMElEQVR4nO3bP2oVURjG4ZPkahFJUCRCvNrYWdgJt09ja2PnEgL2xgUIltaCG3ABVtZTuYoBCeIfRCxCctxAcgvhu0denqf9mrcYhh8Ms9V77w0AINj26AEAANUEDwAQT/AAAPEEDwAQT/AAAPEW644PHz1ui92DTW3ZuAc78+gJpeb9a6MnlLlzmv1z4a+dW6MnlLo4/zJ6Qqmb15ejJ5T62n+OnlDm9v756Amlvm8fjp5Q6uLHaZum6dLb2uBZ7B60e0evS0b9D97vvRw9odSrJ3dHTyhz/PZs9IRSn/aejZ5Q6s/v3PdKa609vX8yekKpd2cfR08o8/zo2+gJpT7cyH42P795ceXNJy0AIJ7gAQDiCR4AIJ7gAQDiCR4AIJ7gAQDiCR4AIJ7gAQDiCR4AIJ7gAQDiCR4AIJ7gAQDiCR4AIJ7gAQDiCR4AIJ7gAQDiCR4AIJ7gAQDiCR4AIJ7gAQDiCR4AIJ7gAQDiCR4AIJ7gAQDiCR4AIJ7gAQDiCR4AIJ7gAQDiCR4AIJ7gAQDiCR4AIJ7gAQDiCR4AIJ7gAQDiCR4AIJ7gAQDiCR4AIJ7gAQDiCR4AIJ7gAQDiCR4AIJ7gAQDiCR4AIJ7gAQDiCR4AIJ7gAQDiCR4AIJ7gAQDiCR4AIJ7gAQDiCR4AIJ7gAQDiCR4AIN5W771fdVytVm25XG5yDwDAP5nnuU3TdOltbfAAACTwSQsAiCd4AIB4ggcAiCd4AIB4ggcAiPcXt94125gKGHEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set styles \n",
    "%matplotlib inline\n",
    "\n",
    "# mpl.style.use('ggplot')\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "sns.set_style('white')\n",
    "sns.set_palette('muted')\n",
    "pylab.rcParams['figure.figsize'] = 16, 10\n",
    "background_color = '#f6f5f5'\n",
    "col_blue = '#4878d0'\n",
    "col_orange = '#ee854a'\n",
    "\n",
    "# Colour palette hex references \n",
    "pal = sns.color_palette('muted')\n",
    "sns.palplot(pal)\n",
    "print(pal.as_hex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'import'></a>\n",
    "## 2.1 Importing the Data\n",
    "\n",
    "#### Varia\n",
    "\n",
    "* Invoice No:\n",
    "* Stock Code:\n",
    "* Description: \n",
    "* Quantity:\n",
    "* Invoice Date:\n",
    "* Unit Price:\n",
    "* Customer ID: \n",
    "* Country:\n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/ecommerce-data/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"general\"></a>\n",
    "\n",
    "## 3. General \n",
    "\n",
    "**Notes from the initial familiarization of the dataset here**\n",
    "\n",
    "* Dataset has X categorical variables: country\n",
    "* Dataset has X continuous variables: quantity, invoicedate, unitprice\n",
    "* Dataset: invoiceno -> unique, stockcode, description, customerid\n",
    "* Dataset has 541,909 rows\n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"shape\"></a>\n",
    "## 3.1 Shape\n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Dataset has {df.shape[0]} rows and {df.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stats'></a>\n",
    "\n",
    "## 3.2 Statistics \n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Familiarization \n",
    "\n",
    "print(df.info())\n",
    "\n",
    "# note that minimum values of price and quantity are negative \n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'hygiene'></a>\n",
    "\n",
    "## 3.3 Hygiene \n",
    "\n",
    "* Convert column names to lowercase\n",
    "* Convert date to datetime format\n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns names to lowercase\n",
    "df.columns = [c.lower() for c in df.columns]    \n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df['invoicedate'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert invoicedate to datetime \n",
    "\n",
    "df['invoicedate'] = pd.to_datetime(df['invoicedate'],\n",
    "                                   errors = 'raise',\n",
    "                                   format = '%m/%d/%Y %H:%M')\n",
    "\n",
    "# check \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort transactions by time \n",
    "df = df.sort_values(by = 'invoicedate', axis = 0, ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'cohort'></a>\n",
    "## 4. Cohort Analysis \n",
    "\n",
    "A cohort is a group of subjects who share a defining characteristic. We can observe how a cohort behaves across time and compare it to other cohorts. Cohorts are used in medicine, psychology, econometrics, ecology and many other areas to perform a cross-section (compare difference across subjects) at intervals through time.\n",
    "\n",
    "**Types of cohorts:**\n",
    "\n",
    "* Time Cohorts are customers who signed up for a product or service during a particular time frame. Analyzing these cohorts shows the customers’ behavior depending on the time they started using the company’s products or services. The time may be monthly or quarterly even daily.\n",
    "* Behavior cohorts are customers who purchased a product or subscribed to a service in the past. It groups customers by the type of product or service they signed up. Customers who signed up for basic level services might have different needs than those who signed up for advanced services. Understaning the needs of the various cohorts can help a company design custom-made services or products for particular segments.\n",
    "* Size cohorts refer to the various sizes of customers who purchase company’s products or services. This categorization can be based on the amount of spending in some periodic time after acquisition or the product type that the customer spent most of their order amount in some period of time.\n",
    "\n",
    "\n",
    "**Approach** \n",
    "* Creating a deep copy for cohort analysis\n",
    "\n",
    "[back to top](#table_of_contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = df.copy(deep = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_cleaning'></a>\n",
    "## 4.1 Data Cleaning\n",
    "\n",
    "**The data cleaning process involves going through and applying the 4Cs to the data**\n",
    "1. Completing - are there any **missing values**? \n",
    " * ~customerid: lots of missing customer ids~ Transactions dropped (cancelled)\n",
    " * ~description: missing description~ Transactions dropped<br></br>\n",
    " \n",
    "2. Correcting - are there any values that **look weird**?  \n",
    " * ~unitprice: There are negative values~ Transactions dropped (likely free gifts)\n",
    " * ~invoiceno: There are values that start with C~ Cancelled values to drop\n",
    " * ~quantity: There are negative values~ Transactions dropped (likely refunded)<br></br>\n",
    "\n",
    "3. Creating - are there any features that we can **engineer** to help solve our problem? \n",
    " * ~invoicedate: Creating month, year, day~\n",
    " * ~totalvalue: quantity x price~<br></br>\n",
    " \n",
    "4. Converting - are all features in the **right dtypes?** \n",
    " * ~Converting invoicedate to datetime~ converted \n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'invoiceno'></a>\n",
    "\n",
    "## 4.2 Invoiceno test\n",
    "\n",
    "**Notes**\n",
    "* Test if all of invoiceno are numeric values: no \n",
    "* Check unique values that start with an alphabet: all start with C\n",
    "* Hypothesis: C represents cancelled orders \n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.to_numeric(dfc['invoiceno'], errors = 'raise')\n",
    "\n",
    "# error generated: Unable to parse string \"C536379\" at position 119"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9291 values \n",
    "odd_invoicenos = [i for i in dfc['invoiceno'] if i[0].isalpha()]\n",
    "print(f'These {len(odd_invoicenos)} invoices are odd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odd_invoicenos[:20]\n",
    "\n",
    "# seem to be all Cs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis: These represent cancelled orders\n",
    "\n",
    "#### Strategy: extract out the C-invoices and check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['cancelled'] = dfc['invoiceno'].apply(lambda x: 1 if x[0] == 'C' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All of the cancelled items seem to have negative quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.loc[dfc['cancelled'] == 1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether there's any non negative\n",
    "len(dfc.loc[(dfc['cancelled'] == 1) & (dfc['quantity'] >= 0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'quantityprice'></a>\n",
    "\n",
    "## 4.3 Quantity / Price Test\n",
    "\n",
    "**Notes**\n",
    "* Test if quantities and price are all above 0\n",
    "* Check for non-cancelled orders that have quantities / price < 0\n",
    "* Hypothesis: these are orders that have gone missing / broken somehow, therefore, their cost has been refunded\n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_quantities = dfc.loc[(dfc['quantity'] < 0) & (dfc['cancelled'] == 0)]\n",
    "neg_quantities.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptions have NaN values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {neg_quantities['description'].isna().sum()} NA values for description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative quantities have various refund reasons \n",
    "neg_quantities['description'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for unitprice equals 0: these are most likely free items as part of a promotional offer / giveaway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.loc[dfc['unitprice'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refund as an adjustment to bad debt \n",
    "dfc.loc[dfc['unitprice'] < 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'iddesc'></a>\n",
    "## 4.4 Completing Variables\n",
    "\n",
    "**Managing missing customerids and description** \n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(dfc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total missing values \n",
    "print(f'There are {dfc.isna().sum().sum()} missing values')\n",
    "\n",
    "# missing values by column \n",
    "dfc.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.dropna(subset = ['description', 'customerid'], axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop cancelled transactions, transactions where the unitprice is <= 0, transactions where quantity <= 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = dfc[(dfc['cancelled'] == 0) & (dfc['unitprice'] > 0) & (dfc['quantity'] > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for duplicated values and drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for customerid duplicates and transaction duplicates\n",
    "print(f\"There are {dfc['customerid'].duplicated().sum()} repeat orders\")\n",
    "print(f'There are {dfc.duplicated().sum()} duplicates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {dfc.duplicated().sum()} duplicates')\n",
    "print(f'Dataset has {dfc.shape[0]} rows and {dfc.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check stats for dfc\n",
    "\n",
    "print(dfc.info())\n",
    "dfc.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'creating'></a>\n",
    "## 4.5 Creating new variables  \n",
    " \n",
    "* get invoicedates grouped by customers\n",
    "* extract first transaction period\n",
    "* calculate time offset in days\n",
    "* calculate total value\n",
    "* make cohort tables: retention, price, quantity  \n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a invoicedatetime column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['invoicedatetime'] = dfc['invoicedate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a totalvalue column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['totalvalue'] = dfc['unitprice'] * dfc['quantity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will parse the dates \n",
    "def get_date(x): return dt.datetime(x.year, x.month, x.day)\n",
    "\n",
    "# Define a function to extract the time from the datetime value\n",
    "def get_time(x): return dt.time(x.hour, x.minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['invoicedate'] = dfc['invoicedatetime'].apply(get_date)\n",
    "dfc['invoicetime'] = dfc['invoicedatetime'].apply(get_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time difference of about a year, which is compatible for a monthly cohort analysis > ~12 cohorts\n",
    "print(f'Earliest date: {dfc[\"invoicedate\"].dt.date.min()}')\n",
    "print(f'Latest date: {dfc[\"invoicedate\"].dt.date.max()}')\n",
    "dfc['invoicedate'].value_counts().sort_index().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month(x): return dt.datetime(x.year, x.month, 1)\n",
    "\n",
    "dfc['invoicemonth'] = dfc['invoicedate'].apply(get_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract first transaction period: cohort join date\n",
    "groupings = dfc.groupby('customerid')['invoicemonth']\n",
    "dfc['cohortdate'] = groupings.transform('min')\n",
    "dfc.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to return the date components as integers\n",
    "def get_date_int(df, col): \n",
    "    year = df[col].dt.year\n",
    "    month = df[col].dt.month\n",
    "    day = df[col].dt.day\n",
    "    return year, month, day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_year, inv_month, _ = get_date_int(dfc, 'invoicemonth')\n",
    "coh_year, coh_month, _ = get_date_int(dfc, 'cohortdate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can calculate the months offset since the first transaction of each customer to arrive at the cohort "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate year difference\n",
    "year_diff = inv_year - coh_year\n",
    "\n",
    "# Calculate month difference\n",
    "month_diff = inv_month - coh_month\n",
    "\n",
    "# Calculate the total time difference in days \n",
    "dfc['cohortindex'] = year_diff * 12 + month_diff + 1\n",
    "\n",
    "dfc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'retention'></a>\n",
    "## 4.6 Calculate Absolute Cohorts and Retention \n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count monthly active customers from each cohort \n",
    "\n",
    "grouping = dfc.groupby(['cohortdate', 'cohortindex'])\n",
    "cohort_data = grouping['customerid'].apply(pd.Series.nunique)\n",
    "\n",
    "display(cohort_data.head())\n",
    "\n",
    "# Pivot by cohortdate and cohortindex to get the number of unique customers that made a purchase by cohort\n",
    "cohort_data = cohort_data.reset_index()\n",
    "absolutes = cohort_data.pivot(index = 'cohortdate', columns = 'cohortindex', values = 'customerid')\n",
    "\n",
    "# Absolute unique customers \n",
    "absolutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the values from the first column \n",
    "cohort_sizes = absolutes.iloc[:, 0]\n",
    "retention = absolutes.divide(cohort_sizes, axis = 0).round(3)*100\n",
    "\n",
    "retention = retention.round(1)\n",
    "retention.index = retention.index.date\n",
    "\n",
    "retention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'price'></a>\n",
    "## 4.7 Calculating Average Price\n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby cohortdate and cohortindex\n",
    "grouping = dfc.groupby(['cohortdate', 'cohortindex']) \n",
    "\n",
    "# Calculate the unitprice average and reset index\n",
    "cohort_data = grouping['unitprice'].mean()\n",
    "cohort_data = cohort_data.reset_index()\n",
    "\n",
    "# Create a price pivot \n",
    "price = cohort_data.pivot(index = 'cohortdate', columns = 'cohortindex', values = 'unitprice')\n",
    "price = price.round(1)\n",
    "price.index = price.index.date\n",
    "\n",
    "price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'quantity'></a>\n",
    "## 4.8 Calculating Average Quantity\n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping = dfc.groupby(['cohortdate', 'cohortindex'])\n",
    "cohort_data = grouping['quantity'].mean()\n",
    "cohort_data = cohort_data.reset_index()\n",
    "\n",
    "# Create the quantity pivot table \n",
    "quantity = cohort_data.pivot(index = 'cohortdate', columns = 'cohortindex', values = 'quantity')\n",
    "quantity = quantity.round()\n",
    "quantity.index = quantity.index.date\n",
    "\n",
    "\n",
    "quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'eda'></a>\n",
    "## 5. Cohort Analysis Visualization  \n",
    "\n",
    "* Price \n",
    "* Quantity \n",
    "* Retention\n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a plotting function \n",
    "\n",
    "def plot_table(df, vmin = 0, vmax = 10, cmap = 'Blues', title = ''):\n",
    "\n",
    "    fig = plt.figure()\n",
    "    gs = fig.add_gridspec(1, 1)\n",
    "    ax0 = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "    sns.heatmap(df, ax = ax0, \n",
    "                annot = True, annot_kws = {'fontsize': 11},\n",
    "                square = False, cbar = True, \n",
    "                vmin = vmin, vmax = vmax, \n",
    "                cmap = cmap, fmt = 'g')\n",
    "\n",
    "    locals()['ax0'].set_title(f'{title}\\n', fontsize = 15, fontweight = 'bold')\n",
    "    locals()['ax0'].set_xlabel('')\n",
    "    locals()['ax0'].set_ylabel('')\n",
    "    locals()['ax0'].set_xticklabels(labels = df.columns, fontsize = 11)\n",
    "    locals()['ax0'].set_yticklabels(labels = df.index, fontsize = 11)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# define unique palettes for each \n",
    "cmap_r = sns.diverging_palette(22, 80, s = 70, l = 70, as_cmap = True)\n",
    "cmap_p = sns.diverging_palette(22, 150, s = 80, l = 55, as_cmap = True)\n",
    "cmap_q = sns.diverging_palette(22, 219, s = 80, l = 55, as_cmap = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retention \n",
    "plot_table(retention, vmin = 20, vmax = 40, cmap = cmap_r, title = '% Retention Rate by Cohort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price \n",
    "plot_table(price, vmin = 1.5, vmax = 5, cmap = cmap_p, title = 'Average Price by Cohort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantity \n",
    "plot_table(quantity, vmin = 5, vmax = 15, cmap = cmap_q, title = 'Average Quantity by Cohort')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'rfm'></a>\n",
    "## 6. RFM Analysis \n",
    "\n",
    "* RFM is an acronym of recency, frequency and monetary. Recency is about when was the last order of a customer. It means the number of days since a customer made the last purchase. If it’s a case for a website or an app, this could be interpreted as the last visit day or the last login time.\n",
    "\n",
    "* Frequency is about the number of purchase in a given period. It could be 3 months, 6 months or 1 year. So we can understand this value as for how often or how many a customer used the product of a company. The bigger the value is, the more engaged the customers are. Could we say them as our VIP? Not necessary. Cause we also have to think about how much they actually paid for each purchase, which means monetary value.\n",
    "\n",
    "* Monetary is the total amount of money a customer spent in that given period. Therefore big spenders will be differentiated with other customers such as MVP or VIP\n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the input features, we will start with RFM features – recency, frequency, monetary\n",
    "* Recency – time since the last customer transaction\n",
    "* Frequency – number of purchases in the observed period\n",
    "* Monetary value – total amount spent in the observed period\n",
    "\n",
    "\n",
    "Process of calculating percentiles:\n",
    "\n",
    "* Sort customers based on that metric\n",
    "* Break customers into a pre-defined number of groups of equal size\n",
    "* Assign a label to each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Earliest invoice date: {dfc[\"invoicedatetime\"].dt.date.min()}')\n",
    "print(f'Most recent invoice date: {dfc[\"invoicedatetime\"].dt.date.max()}')\n",
    "\n",
    "dfc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The last day of purchase is 9 Dec 2011; we can set the snapshot date as one day after the last to calculate the no. of days since last transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the snapshotdate\n",
    "snapshotdate = df['invoicedate'].max() + dt.timedelta(days = 1)\n",
    "\n",
    "snapshotdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RFM metrics\n",
    "rfm = dfc.groupby(['customerid']).agg({'invoicedate': lambda x: (snapshotdate - x.max()).days,\n",
    "                                       'invoiceno': 'count',\n",
    "                                       'totalvalue': 'sum'})\n",
    "\n",
    "rfm.rename(columns = {'invoicedate': 'recency', \n",
    "                      'invoiceno': 'frequency', \n",
    "                      'totalvalue': 'monetaryvalue'}, inplace = True)\n",
    "\n",
    "# Resulting table is conducive for scoring and clustering \n",
    "rfm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'rfm_scoring'></a>\n",
    "\n",
    "## 6.1 RFM Scoring \n",
    "Note That :\n",
    "\n",
    "* We will rate \"Recency\" customer who have been active more recently better than the less recent customer,because each company wants its customers to be recent\n",
    "* We will rate \"Frequency\" and \"Monetary Value\" higher label because we want Customer to spend more money and visit more often(that is different order than recency)\n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating labels for RFM segments \n",
    "r_labels = range(4, 0, -1)\n",
    "f_labels = range(1, 5)\n",
    "m_labels = range(1, 5)\n",
    "\n",
    "# Assign labels to percentile gropus \n",
    "r_groups = pd.qcut(rfm['recency'], q = 4, labels = r_labels)\n",
    "f_groups = pd.qcut(rfm['frequency'], q = 4, labels = f_labels)\n",
    "m_groups = pd.qcut(rfm['monetaryvalue'], q = 4, labels = m_labels)\n",
    "\n",
    "# Create new columns in the rfm df\n",
    "rfm = rfm.assign(r_group = r_groups.values, f_group = f_groups.values, m_group = m_groups.values)\n",
    "\n",
    "# Calculate RFM score and profile \n",
    "\n",
    "def add_rfm(x): return str(int(x['r_group'])) + str(int(x['f_group'])) + str(int(x['m_group']))\n",
    "rfm['rfm_profile'] = rfm.apply(add_rfm, axis = 1)\n",
    "rfm['rfm_score'] = rfm[['r_group', 'f_group', 'm_group']].sum(axis = 1)\n",
    "\n",
    "rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random checks to make sure that the groups are making sense \n",
    "print(rfm.loc[rfm['r_group'] == 1, 'recency'].mean())\n",
    "print(rfm.loc[rfm['r_group'] == 4, 'recency'].mean())\n",
    "\n",
    "print(rfm.loc[rfm['f_group'] == 1, 'frequency'].mean())\n",
    "print(rfm.loc[rfm['f_group'] == 4, 'frequency'].mean())\n",
    "\n",
    "print(rfm.loc[rfm['m_group'] == 1, 'monetaryvalue'].mean())\n",
    "print(rfm.loc[rfm['m_group'] == 4, 'monetaryvalue'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'rfm_profiling'></a>\n",
    "\n",
    "## 6.2 RFM Profiling\n",
    "\n",
    "#### Largest RFM segments It is always the best practice to investigate the size of the segments before you use them for targeting or other business Application\n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm.groupby(['rfm_profile']).size().sort_values(ascending = True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering by rfm profiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select bottom RFM segment \"111\" and view top 5 rows\n",
    "rfm[rfm['rfm_profile'] == '111'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'rfm_level'></a>\n",
    "\n",
    "## 6.3 RFM Levels \n",
    "\n",
    "**Creating custom segments using the RFM score as a benchmark**\n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm['rfm_level'] = pd.qcut(rfm['rfm_score'], q = 3, labels = ['low', 'middle', 'high'])\n",
    "\n",
    "rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm.groupby('rfm_level').agg({'recency': 'mean',\n",
    "                              'frequency': 'mean',\n",
    "                              'monetaryvalue': ['mean', 'count']}).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'segmentation'></a>\n",
    "\n",
    "## 7. Segmentation\n",
    "\n",
    "**We must check these key assumptions before we implement our K-Means algorithm**\n",
    "\n",
    "* Symmetric distribution of variables (not skewed) - solution is to log-transform the variables  \n",
    "* Variables with same average values and variance - solution is to standardize the variables \n",
    "* These assumptions can be check through the distplot / kdeplot \n",
    "* In the case of the ecommerce data, which is both skewed and of different scales, log-transformation must be done before scaling \n",
    "* The reason for this is that log-transformations only work with positive values\n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_raw = rfm[['recency','frequency','monetaryvalue']]\n",
    "rfm_raw.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfm_kde(df): \n",
    "    fig = plt.figure(figsize = (16, 25))\n",
    "    gs = fig.add_gridspec(len(df.columns), 1)\n",
    "    gs.update(wspace = 0.2, hspace = 0.2)\n",
    "\n",
    "    run = 0 \n",
    "\n",
    "    for row in range(len(df.columns)):\n",
    "        for col in range(1):\n",
    "            locals()[f'ax{run}'] = fig.add_subplot(gs[row, col])\n",
    "            locals()[f'ax{run}'].set_yticklabels([])\n",
    "            locals()[f'ax{run}'].tick_params(axis = 'y', which = 'both', length = 0)\n",
    "            for s in ['top', 'right', 'left']:\n",
    "                locals()[f'ax{run}'].spines[s].set_visible(False)\n",
    "            run += 1\n",
    "\n",
    "    run = 0\n",
    "\n",
    "    for col in df:\n",
    "        sns.kdeplot(df[col], ax = locals()[f'ax{run}'], shade = True, linewidth = 0.5)\n",
    "        locals()[f'ax{run}'].grid(which = 'major', axis = 'x', zorder = 0, color = 'black', linestyle = ':', dashes = (1,5))\n",
    "        locals()[f'ax{run}'].set_title(col, fontsize = 15, fontweight = 'bold')\n",
    "        locals()[f'ax{run}'].set_xlim(df[col].min() - 2, df[col].max() + 2)\n",
    "        locals()[f'ax{run}'].set_xlabel('')\n",
    "        locals()[f'ax{run}'].set_ylabel('')\n",
    "        run += 1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_kde(rfm_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unskew the data with np.log\n",
    "rfm_log = np.log(rfm_raw)\n",
    "\n",
    "rfm_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the variables with StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "rfm_log_norm = scaler.fit_transform(rfm_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_log_norm = pd.DataFrame(rfm_log_norm, columns = rfm_raw.columns, index = rfm_raw.index)\n",
    "\n",
    "rfm_log_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distributions of the rfm variables \n",
    "rfm_kde(rfm_log_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'kmeans'></a>\n",
    "\n",
    "## 7.1 K-Means Clustering and Visualization¶\n",
    "\n",
    "**Key steps**\n",
    "\n",
    "* Data pre-processing\n",
    "* Choosing a number of clusters\n",
    "* Running k-means clustering on pre-processed data\n",
    "* Analyzing average RFM values of each cluster\n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'choosing_k'></a>\n",
    "\n",
    "## 7.2 Choosing K\n",
    "\n",
    "**Methods to define the number of clusters**\n",
    "\n",
    "* Visual confirmation using the scree plot - identify elbow \n",
    "* Mathematically using the silhouette coefficient\n",
    "* Plot the number of clusters against within-cluster sum-of-squared-errors (SSE) - sum of squared distances from every data point to their cluster center\n",
    "* Identify an \"elbow\" in the plot\n",
    "* Elbow - a point representing an \"optimal\" number of clusters\n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Instantiate multiple values of K and record their within-cluster SSEs \n",
    "\n",
    "K = list(range(1, 10))\n",
    "sse = {}\n",
    "\n",
    "for k in K:\n",
    "    # Create a KMeans trainer\n",
    "    kmeans = KMeans(n_clusters = k, random_state = 123)\n",
    "    kmeans.fit(rfm_log_norm)\n",
    "    sse[k] = kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'screeplot'></a>\n",
    "\n",
    "## 7.3 Scree Plot\n",
    "\n",
    "**Methods to define the number of clusters**\n",
    "\n",
    "* Visual confirmation using the scree plot - identify elbow \n",
    "* Mathematically using the silhouette coefficient\n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "gs = fig.add_gridspec(1, 1)\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "for s in ['top', 'right', 'left', 'bottom']: \n",
    "    ax0.spines[s].set_visible(False)\n",
    "\n",
    "sns.pointplot(x = list(sse.keys()), y = list(sse.values()), ax = ax0, color = col_blue)\n",
    "\n",
    "ax0.set_title('Scree Plot (KMeans)\\n', fontsize = 15, fontweight = 'bold')\n",
    "\n",
    "ax0.set_xlabel('K', fontsize = 15)\n",
    "ax0.set_ylabel('SSE', fontsize = 15)\n",
    "ax0.set_xticklabels(K, fontsize = 15)\n",
    "ax0.set_yticklabels('')\n",
    "\n",
    "ax0.grid(which = 'major', axis = 'y', zorder = 0, color = 'black', linestyle = ':', dashes = (1,5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'k_clusters'></a>\n",
    "\n",
    "## 7.4 K-Means Clusters\n",
    "\n",
    "**Methods to define the number of clusters**\n",
    "\n",
    "* Visual confirmation using the scree plot - identify elbow \n",
    "* Mathematically using the silhouette coefficient\n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection of k for clustering: 3\n",
    "\n",
    "kmeans3 = KMeans(n_clusters = 3, random_state = 1)\n",
    "kmeans3.fit(rfm_log_norm)\n",
    "\n",
    "cluster_labels = kmeans3.labels_\n",
    "\n",
    "rfm_k3 = rfm.assign(k_cluster = cluster_labels)\n",
    "\n",
    "#Calculate average RFM values and sizes for each cluster:\n",
    "rfm_k3.groupby('k_cluster').agg({'recency': 'mean',\n",
    "                                 'frequency': 'mean',\n",
    "                                 'monetaryvalue': ['mean', 'count']}).round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'snake_plots'></a>\n",
    "\n",
    "## 7.5 Snake Plots\n",
    "\n",
    "* plots to understand and compare segments\n",
    "* Market research technique to compare different segments\n",
    "* Visual representation of each segment's attributes\n",
    "* Need to first normalize data (center & scale)\n",
    "* Plot each cluster's average normalized values of each attribute\n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_snake = pd.DataFrame(rfm_log_norm, columns = rfm_log_norm.columns, index = rfm_log_norm.index)\n",
    "rfm_snake['k_cluster'] = kmeans3.labels_\n",
    "rfm_snake = rfm_snake.assign(rfm_level = rfm['rfm_level'])\n",
    "rfm_snake = rfm_snake.reset_index()\n",
    "\n",
    "rfm_snake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the data into a long format\n",
    "rfm_snake = pd.melt(rfm_snake, id_vars = ['customerid', 'rfm_level', 'k_cluster'], \n",
    "                    value_vars = ['recency', 'frequency', 'monetaryvalue'],\n",
    "                    var_name = 'metric', value_name = 'value')\n",
    "\n",
    "rfm_snake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 8))\n",
    "gs = fig.add_gridspec(1, 2)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "for s in ['top', 'right', 'left', 'bottom']: \n",
    "    ax0.spines[s].set_visible(False)\n",
    "    ax1.spines[s].set_visible(False)\n",
    "\n",
    "\n",
    "sns.lineplot(x = 'metric', y = 'value', hue = 'rfm_level', data = rfm_snake, ax = ax0, palette = 'muted')\n",
    "\n",
    "# a snake plot with K-Means\n",
    "sns.lineplot(x = 'metric', y = 'value', hue = 'k_cluster', data = rfm_snake, ax = ax1, palette = 'muted')\n",
    "\n",
    "run = 0\n",
    "\n",
    "for _ in range(2): \n",
    "    locals()[f'ax{run}'].legend(title = 'Cluster', frameon = False)\n",
    "    locals()[f'ax{run}'].set_xlabel('')\n",
    "    locals()[f'ax{run}'].set_ylabel('')\n",
    "    locals()[f'ax{run}'].set_xticks(np.arange(3))\n",
    "    locals()[f'ax{run}'].set_xticklabels(labels = ['Recency', 'Frequency', 'MonetaryValue'])\n",
    "    locals()[f'ax{run}'].set_yticklabels('')\n",
    "    run += 1\n",
    "\n",
    "locals()['ax0'].set_title('Snake Plot of RFM\\nPercentile Clusters', fontsize = 14)\n",
    "locals()['ax1'].set_title('Snake Plot of RFM\\nKMeans Clusters', fontsize = 14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'importance'></a>\n",
    "\n",
    "## 7.6 Importance Scores\n",
    "\n",
    "* Useful technique to identify relative importance of each segment's attribute\n",
    "* Calculate average values of each cluster\n",
    "* Calculate average values of population\n",
    "* Calculate importance score by dividing them and subtracting 1 (ensures 0 is returned when cluster average equals population average)\n",
    "* Let’s try again with a heat map. Heat maps are a graphical representation of data where larger values were colored in darker scales and smaller values in lighter scales. We can compare the variance between the groups quite intuitively by colors.\n",
    "\n",
    "[back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_avg = rfm_k3.groupby(['k_cluster'])[['recency', 'frequency', 'monetaryvalue']].mean()\n",
    "pop_avg = rfm_k3[['recency','frequency', 'monetaryvalue']].mean()\n",
    "\n",
    "importance = cluster_avg / pop_avg - 1\n",
    "importance = importance.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "gs = fig.add_gridspec(1, 1)\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "sns.heatmap(importance, ax = ax0, \n",
    "            annot = True, annot_kws = {'fontsize': 12},\n",
    "            square = True, cbar = True, \n",
    "            vmin = -1, vmax = 1, \n",
    "            cmap = cmap_q, fmt = 'g')\n",
    "\n",
    "ax0.set_title('Importance Scores of RFM Values by Clusters\\n', fontsize = 15, fontweight = 'bold')\n",
    "ax0.set_xticklabels(importance.columns, fontsize = 12, rotation = 0)\n",
    "ax0.set_ylabel('k clusters', rotation = 90)\n",
    "ax0.set_yticklabels(labels = [0, 1, 2], rotation = 0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: We talked about how to get RFM values from customer purchase data, and we made two kinds of segmentation with RFM quantiles and K-Means clustering methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Market Basket Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we go grocery shopping, we often have a standard list of things to buy. Each shopper has a distinctive list, depending on one’s needs and preferences. A housewife might buy healthy ingredients for a family dinner, while a bachelor might buy beer and chips. Understanding these buying patterns can help to increase sales in several ways. If there is a pair of items, X and Y, that are frequently bought together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlxtend'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-87d910befb79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmlxtend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mlxtend'"
     ]
    }
   ],
   "source": [
    "import mlxtend missing? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-6507963134d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdfm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "dfm = df.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-8d2ed49b8fe8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdfm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'invoicedate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dfm' is not defined"
     ]
    }
   ],
   "source": [
    "dfm.set_index(['invoicedate'], inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-8f9e2c523973>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'The dataset has {dfm.shape[0]} rows and {dfm.shape[1]} columns'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dfm' is not defined"
     ]
    }
   ],
   "source": [
    "print(f'The dataset has {dfm.shape[0]} rows and {dfm.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(dfm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for customer id \n",
    "dfm.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without these descriptions, we wouldn't be able to compute any association rules \n",
    "dfm.loc[dfm['description'].isna(), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm.dropna(subset = ['description', 'customerid'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm['cancelled'] = dfm['invoiceno'].apply(lambda x: 1 if x[0] == 'C' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop cancelled orders\n",
    "dfm = dfm[dfm['cancelled'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = dfm[(dfm['quantity'] > 0) & (dfm['unitprice'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm['invoiceno'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop transactions of a single item\n",
    "inv = pd.DataFrame(dfm['invoiceno'].value_counts())\n",
    "\n",
    "multi_inv = inv[inv['invoiceno'] > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_inv.index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = dfm[dfm['invoiceno'].isin(multi_inv.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequent sets and association rules with apriori:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm['description'] = dfm['description'].str.strip().str.lower()\n",
    "dfm.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket = pd.get_dummies(dfm.loc[:, 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket = basket.assign(invoiceno = dfm['invoiceno'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket_sets = basket.groupby('invoiceno').agg(np.max, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apriori Algorithm: Frequent Itemsets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that min_support parameter was set to a very low value, this is the Spurious limitation, more on conclusion section\n",
    "frequent_itemsets = apriori(basket_sets, min_support=0.03, use_colnames=True)\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "\n",
    "# Advanced and strategical data frequent set selection\n",
    "frequent_itemsets[ (frequent_itemsets['length'] > 1) &\n",
    "                   (frequent_itemsets['support'] >= 0.02) ].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the association_rules: rules\n",
    "# Selecting the important parameters for analysis\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "rules[['antecedants', 'consequents', 'support', 'confidence', 'lift']].sort_values('support', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the rules distribution color mapped by Lift\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(rules['support'], rules['confidence'], c=rules['lift'], alpha=0.9, cmap='YlOrRd');\n",
    "plt.title('Rules distribution color mapped by lift');\n",
    "plt.xlabel('Support')\n",
    "plt.ylabel('Confidence')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "Potential of the solution\n",
    "\n",
    "* Implementation simplicity\n",
    "* Once again, the implementation of the solution in terms of code is simple. Deployement in most cases can be report based exporting the relevant rules for discussion.\n",
    "\n",
    "* Statistical interpretation\n",
    "* The statistical interpretation of how support, confidence and lift can correlate with marketing strategies take some time and know how on the field where it's being applied. Start here for explanations about the output attributes from this model.\n",
    "\n",
    "* Apriori limitations\n",
    "\n",
    "* As seen on the KDNuggets article referenced in the Introduction, we faced the Spurious Associations limitation. This happend due to the eCommerce business model, a large number of possibilities in a single basket among an even larger number of baskets. The consequence of it is having a \"sparse matrix\", full of 0s which causes the support of basket occourances to drop drastically. The output achieved has its top support of 0.051 (5%). Such limitation might be overcome by working with the entire data set, remember that only 30000 top rows were analysed, or this could dilute the support values even more. As a last optio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
